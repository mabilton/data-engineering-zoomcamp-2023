{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a9dbc9-8623-4476-9ab6-8ab90a173e3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homework 5 Answers\n",
    "\n",
    "This Jupyter notebook contains all of my answers to the Week 5 homework questions for the 2023 cohort of the Data Engineering Zoomcamp. For convenience, each question is restated before giving the corresponding answer; the list of questions *without* corresponding answers can be found in `01-questions.md`, which can be found in the current directory.\n",
    "\n",
    "Please note that this notebook **assumes that the `05-batch/homework` directory is your present working directory**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef3f8d-2aff-4a43-8dc0-9277faf766c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd53bcb-6c24-4a14-ad7c-2c1ea5408e15",
   "metadata": {},
   "source": [
    "First, let's install all of the requirements we'll need to run this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c36984e-1b20-4015-8469-9e84339a4e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas~=1.5.2 in /home/mabilton/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: pyspark~=3.3.2 in /home/mabilton/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: wget~=3.2 in /home/mabilton/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mabilton/.local/lib/python3.10/site-packages (from pandas~=1.5.2->-r requirements.txt (line 1)) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/mabilton/.local/lib/python3.10/site-packages (from pandas~=1.5.2->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/mabilton/.local/lib/python3.10/site-packages (from pandas~=1.5.2->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/mabilton/.local/lib/python3.10/site-packages (from pyspark~=3.3.2->-r requirements.txt (line 2)) (0.10.9.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/mabilton/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas~=1.5.2->-r requirements.txt (line 1)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c84d4-9a8e-417a-959b-37b5e0feedb4",
   "metadata": {},
   "source": [
    "Note that you'll need to make sure that `spark` is installed in order for `pyspark` to work; please refer to the [instructions provided in the Data Engineering Zoomcamp GitHub repository](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_5_batch_processing/setup) for how to install `spark` on your particular machine.\n",
    "\n",
    "Next, let's import the modules we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b3517e-061c-43d1-b9aa-7b21910422e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types\n",
    "\n",
    "# See `utils.py` for descriptions of `download_csv` and `get_parquet_file_sizes`:\n",
    "from utils import download_csv, get_parquet_file_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0714b6-6b99-4db8-b5ea-d03801a85976",
   "metadata": {},
   "source": [
    "Let's now download the two CSV files we need to answer the homework questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f454f41f-84d7-486f-88ce-efff2c0beefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_csv_path = download_csv(\n",
    "    \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\"\n",
    ")\n",
    "zone_lookup_path = download_csv(\n",
    "    \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33767ee3-a2a4-40c5-8597-db9c77613af9",
   "metadata": {},
   "source": [
    "We'll now start our `spark` session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b306711e-fa2f-4e25-a6f2-7705a1027c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"zoomcamp_hw5\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51a7e0-3e93-4c38-86bd-f0bbdef80537",
   "metadata": {},
   "source": [
    "If you so desire, you can monitor the progress of the jobs we send to our `spark` cluster at the following port:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b24178ea-d2fb-4483-81c5-c425fb7ceeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://192.168.1.126:4040'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342313f-b8e3-4e35-8dd1-0b5044ba5834",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38ebf4-9587-4bfd-b73e-6b5e41cda263",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Create a `spark` session with `pyspark` and call `spark.version`. What's the output? Choose from the following options:\n",
    "- 3.3.2\n",
    "- 2.1.4\n",
    "- 1.2.3\n",
    "- 5.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be1903-124e-4a57-b0d8-81c148593fae",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Executing `spark.version` shows that we're running version `3.3.2` of `pyspark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e35b695-8a15-46ac-853c-4bb943bff7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6f308-4c38-4c48-b634-5d677f5ec986",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee0b5b-6027-425a-900e-3a881f59c898",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Create a `pyspark` dataframe for the downloaded FHVHV CSV into `spark`, and repartition it into 12 partitions. After this, save the data to Parquet. What is the average size of the 12 created Parquet files (i.e. the files ending with a `.parquet` extension)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92235f7e-2b6e-41b8-88f6-5f798cb40e7e",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Before loading the FHVHV CSV into `spark`, we first need to work out the schema for this data. To help us do this, let's use `pandas` to load the first five rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8a01f1-23d9-4bd9-8257-6d72c28cf8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:02:41</td>\n",
       "      <td>2021-06-01 00:07:46</td>\n",
       "      <td>174</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:16:16</td>\n",
       "      <td>2021-06-01 00:21:14</td>\n",
       "      <td>32</td>\n",
       "      <td>254</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:27:01</td>\n",
       "      <td>2021-06-01 00:42:11</td>\n",
       "      <td>240</td>\n",
       "      <td>127</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-06-01 00:46:08</td>\n",
       "      <td>2021-06-01 00:53:45</td>\n",
       "      <td>127</td>\n",
       "      <td>235</td>\n",
       "      <td>N</td>\n",
       "      <td>B02764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B02510</td>\n",
       "      <td>2021-06-01 00:45:42</td>\n",
       "      <td>2021-06-01 01:03:33</td>\n",
       "      <td>144</td>\n",
       "      <td>146</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropoff_datetime  \\\n",
       "0               B02764  2021-06-01 00:02:41  2021-06-01 00:07:46   \n",
       "1               B02764  2021-06-01 00:16:16  2021-06-01 00:21:14   \n",
       "2               B02764  2021-06-01 00:27:01  2021-06-01 00:42:11   \n",
       "3               B02764  2021-06-01 00:46:08  2021-06-01 00:53:45   \n",
       "4               B02510  2021-06-01 00:45:42  2021-06-01 01:03:33   \n",
       "\n",
       "   PULocationID  DOLocationID SR_Flag Affiliated_base_number  \n",
       "0           174            18       N                 B02764  \n",
       "1            32           254       N                 B02764  \n",
       "2           240           127       N                 B02764  \n",
       "3           127           235       N                 B02764  \n",
       "4           144           146       N                    NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(fhvhv_csv_path, nrows=5)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b8456-d8ea-423a-9d20-a0f2da8d00ff",
   "metadata": {},
   "source": [
    "From this output, it appears that the FHVHV dataset has the following schema:\n",
    "- `dispatching_base_num` is a string\n",
    "- `pickup_datetime` is a timestamp\n",
    "- `dropoff_datetime\t` is a timestamp\n",
    "- `PULocationID` is a (short) integer\n",
    "- `SR_Flag` is a string\n",
    "- `Affiliated_base_number` is a string\n",
    "\n",
    "We can specify this schema in `pyspark` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f253151b-2999-4290-abed-67c0d15c2fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "        types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"dropoff_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "        types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "        types.StructField(\"SR_Flag\", types.StringType(), True),\n",
    "        types.StructField(\"Affiliated_base_number\", types.StringType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55174c8c-7821-4016-93ab-aadfe56f4720",
   "metadata": {},
   "source": [
    "With the schema specified, let's now load the CSV data as a `pyspark` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7944fd4-c185-435a-bee4-0ece9bca1953",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(fhvhv_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dac64-2a6b-476d-919c-3b18256dccbf",
   "metadata": {},
   "source": [
    "We can now repartition this dataframe into 12 separate partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cba9afa-0dae-4139-b724-c16365bcda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefa110-8ee2-4b29-a449-9457ff97a51b",
   "metadata": {},
   "source": [
    "Let's now save this reparatitioned table into `parquet` format; we'll store the saved `parquet` files in a subdirectory called `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85dac40f-8f1a-48bd-8bb1-ddc3c2ee7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"data/\"):\n",
    "    os.rmtree(\"data/\")\n",
    "df.write.parquet(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd374ea-de34-47f8-8a0c-1f032631a476",
   "metadata": {},
   "source": [
    "We'll now use the `get_parquet_file_sizes` utility function we've defined in `utils.py` to list the file sizes of all the `.parquet` files we've saved in the `data` subdirectory; note that all of the listed files sizes are in **MB**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94d3fa4-52ee-45e7-a6f4-1dd2b839773e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'part-00002-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.678828239440918,\n",
       " 'part-00001-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.666645050048828,\n",
       " 'part-00011-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.66179370880127,\n",
       " 'part-00005-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.67503070831299,\n",
       " 'part-00004-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.67267417907715,\n",
       " 'part-00000-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.674582481384277,\n",
       " 'part-00010-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.666647911071777,\n",
       " 'part-00009-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.675537109375,\n",
       " 'part-00003-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.673087120056152,\n",
       " 'part-00007-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.67320442199707,\n",
       " 'part-00008-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.671749114990234,\n",
       " 'part-00006-082d240a-463d-4186-95c4-3d986a7c15b9-c000.snappy.parquet': 23.67037296295166}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_parquet_file_sizes(dir_to_search=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c6e2a-d039-4430-abc2-b860586c1fdc",
   "metadata": {},
   "source": [
    "It appears that the average size of the Parquet files we've just created is around **24 MB**.\n",
    "\n",
    "Now that we've created these Parquet files, it makes sense to use them to recreate our dataframe; this should speed-up our queries for subsequent questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa7cf40-8c0b-4e65-a83e-ef4b4f907670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb6d7a-2963-488a-a93d-4500e5892030",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3555e9c-7bf7-464b-a960-644787cb6e0a",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "How many taxi trips started on June 15 in the FHVHV dataset you downloaded?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d70309-ffec-42c9-9695-af8478f83040",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "This question is probably most 'cleanly' answered by using a SQL query, so let's first tell `spark` to create a table called `trips_data` from our `df` which we can query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7af3f2b3-0f08-493a-9b53-51a4bf1b9588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"trips_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d5b76-2382-4097-9a32-628184ba391b",
   "metadata": {},
   "source": [
    "We can now use the following simple SQL query to count the number of drips occurring on `2021-06-15`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e762906-7041-49b9-9916-252d80db9f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "WHERE\n",
    "    DATE(pickup_datetime) = '2021-06-15';\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc50e8f-ca8b-41f9-8a93-ab8c70701bc0",
   "metadata": {},
   "source": [
    "i.e. `452470` trips occurred on `2021-06-15`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833bb8e1-a76c-41c2-b3a3-c22bab2295b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7f408-e32a-4043-ae90-fb1c653b0bda",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "How much time (measured in hours) did the longest taxi trip in the dataset take?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c90078-7eff-4b76-aa3a-4937ecd926e9",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "We can find length of the longest trip in our FHV dataframe in three steps:\n",
    "1. Compute the time of each trip in seconds by subtracting the UNIX timestampe of the `dropoff_datetime` from the UNIX timestampe of the `pickup_datetime`. For those who are unaware, the UNIX timestamp of a particular time measures the number of seconds that have elapsed between X and that particular time, which means that the difference between two UNIX timestamps is simply the seconds that have elapsed between those two times.\n",
    "2. Compute the time of each trip in hours by dividing the previously calculated trip time by 3600 (i.e. there are 3600 seconds in an hour).\n",
    "3. Return the maximum trip time in hours.\n",
    "\n",
    "Doing all this shows us that the longest FHVHV trip in June 2021 took around **67 hours**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c654b8fc-ed60-4dba-a05e-ca2b77714eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|max(trip_time_in_hrs)|\n",
      "+---------------------+\n",
      "|     66.8788888888889|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    \"trip_time_in_secs\",\n",
    "    F.unix_timestamp(df.dropoff_datetime) - F.unix_timestamp(df.pickup_datetime),\n",
    ").withColumn(\"trip_time_in_hrs\", F.col(\"trip_time_in_secs\") / 3600).select(\n",
    "    F.max(\"trip_time_in_hrs\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa83e40-79f9-4b0e-bf0c-8bbbb08c165b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979b467-9a76-4fde-9ec2-0d0eaf0e6d81",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Which port on your local machine do you use to access the Spark User Interface dashboard?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666864c1-079e-417b-bfd6-44d5fc49d5b5",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "As we previously shown near the start of this notebook, we can use `pyspark` to get a link to the Spark User Interface dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c4879ff-fa30-4437-917c-e1b5755780b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://192.168.1.126:4040'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e2453-0f21-46bb-a23f-7455d2953371",
   "metadata": {},
   "source": [
    "From this link, we can clearly see that the Spark User Interface runs on **Port 4040** of our local system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6491ec-5724-4777-b075-9f4bfd5bdd97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7ba5d4-6510-46b3-8fe0-684b8ce58804",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "With the help of the Taxi Zone Lookup table, find the name of the most frequent pickup zone location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e867f5-97d4-423c-8b52-7f0e4c437be2",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Let's first create a `pyspark` dataframe for the Taxi Zone lookup data we've downloaded. To do this, we need to work out the schema of this data. Once again, we'll work out the schema to specify for this data by using `pandas` to read the first fice rows of the CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13f7dda1-505a-4f7e-a795-cfa91ec173be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID        Borough                     Zone service_zone\n",
       "0           1            EWR           Newark Airport          EWR\n",
       "1           2         Queens              Jamaica Bay    Boro Zone\n",
       "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
       "3           4      Manhattan            Alphabet City  Yellow Zone\n",
       "4           5  Staten Island            Arden Heights    Boro Zone"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones = pd.read_csv(zone_lookup_path, nrows=5)\n",
    "df_zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b110f1b-9041-40f2-ae14-8c54a0df7d26",
   "metadata": {},
   "source": [
    "From this, we see that this data uses the following schema:\n",
    "- `LocationID` is a (short) integer\n",
    "- `Borough` is a string\n",
    "- `Zone` is a string\n",
    "- `service_zone` is a string\n",
    "\n",
    "We can specify this schema in `pyspark` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93a5f1e0-9acc-4cf7-acf5-fda62825f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"LocationID\", types.ShortType(), False),\n",
    "        types.StructField(\"Borough\", types.StringType(), False),\n",
    "        types.StructField(\"Zone\", types.StringType(), False),\n",
    "        types.StructField(\"service_zone\", types.StringType(), False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25688518-77a1-49a5-8aaa-661293678c2b",
   "metadata": {},
   "source": [
    "With the schema specified, let's now read in this CSV to `pyspark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be41d56b-42d4-402a-8c3d-1c716eab550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read.option(\"header\", \"true\").schema(schema).csv(zone_lookup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4dcc0e-8d34-4966-a28c-9c64f64c0b9a",
   "metadata": {},
   "source": [
    "Instead of merging the FHV dataframe `df` and the Taxi zone dataframe `df_zone`, it's more computationally efficient (and also cleaner/easier, in my personal opinion) to answer this question with a SQL query that uses a Common Table Expression (CTE). \n",
    "\n",
    "To do this, we'll first need to tell `pyspark` to create a table for the `df_zone` dataframe, which we'll call `zone_lookup`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c299075-a6d5-41ad-bec5-9f96c2d52f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone.createOrReplaceTempView(\"zone_lookup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208054b-240b-46c1-8938-c4039c4f8296",
   "metadata": {},
   "source": [
    "We can then query both the `zone_lookup` we've just created, as well as the `trips_data` table we previously created, to answer our question. More specifically, we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b55e65f-d66a-4ce9-805e-dc5e526cb22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|               Zone|num_pickups|\n",
      "+-------------------+-----------+\n",
      "|Crown Heights North|     231279|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "-- Count number of trips starting in each zone and keep only zone with most trips:\n",
    "WITH most_freq_pu AS (\n",
    "    SELECT \n",
    "        PULocationID, \n",
    "        COUNT(1) AS num_pickups\n",
    "    FROM \n",
    "        trips_data\n",
    "    GROUP BY \n",
    "        PULocationID\n",
    "    ORDER BY \n",
    "        num_pickups DESC\n",
    "    LIMIT \n",
    "        1\n",
    ")\n",
    "-- Find corresponding zone name:\n",
    "SELECT \n",
    "    zl.Zone, pu.num_pickups\n",
    "FROM\n",
    "    zone_lookup AS zl,\n",
    "    most_freq_pu AS pu\n",
    "WHERE\n",
    "    zl.LocationID = pu.PULocationID;\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552430d5-55ab-4ee4-9a25-4bc8b3c4a943",
   "metadata": {},
   "source": [
    "We see here that the **Crown Heights North** zone had the most FHVHV taxi trip pick-ups in June 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc55b9-5f50-4d36-95cc-8eace7841c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
