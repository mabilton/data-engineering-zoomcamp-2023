{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1d4736-b3a4-4bc2-97d5-3cd4e5636abb",
   "metadata": {},
   "source": [
    "# Homework - Question 6 Code\n",
    "\n",
    "This notebook contains ; please refer to 'Part 2' of `02-answers.md` for further details. Please note that all of the code cells in this notebook assume that you're running the notebook **within the `06-stream/homework` directory**.\n",
    "\n",
    "\n",
    "Before running any of the code in this notebook, make sure that you've:\n",
    "1. Started up a local Kafka cluster by executing `docker compose up -d` inside of a terminal instance that is located within the `06-stream/homework` directory.\n",
    "1. Install all the Python requirements by executing `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8c668-50e1-4278-8c7c-75085c6cde30",
   "metadata": {},
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4cfcde-621b-4d5b-b704-9a1600e559d0",
   "metadata": {},
   "source": [
    "First, let's import the modules we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff355ae1-4aa2-4ba5-aaae-6dc6944ab50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#\n",
    "from producers import confluent_producer, kafka_python_producer\n",
    "from consumers import confluent_consumer, kafka_python_consumer\n",
    "from utils import download_csv, load_kafka_settings\n",
    "from streaming import (\n",
    "    parse_taxi_messages,\n",
    "    show_streaming_df,\n",
    "    write_df_to_topic,\n",
    "    read_df_from_kafka,\n",
    "    prepare_df_for_producing,\n",
    "    count_streaming_df_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc3d0c-2c2c-469f-bc1f-653ef3f1c404",
   "metadata": {},
   "source": [
    "Next, we'll download the Green and FHV taxi CSV files specified in the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52169fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_csv_path = download_csv(\n",
    "    \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-01.csv.gz\"\n",
    ")\n",
    "fhv_csv_path = download_csv(\n",
    "    \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-01.csv.gz\"\n",
    ")\n",
    "TAXI_CSVS = {\"green\": green_csv_path, \"fhv\": fhv_csv_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e01e7-5f4d-400d-88bc-94d7c33e801f",
   "metadata": {},
   "source": [
    "Let's now load `kafka_settings.yaml`, which lists the configurations settings associated with the Kafka cluster running in Docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b5e68b-f179-446a-97d8-b0e4c957fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = load_kafka_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc10a8-7387-427f-b080-157fdaac102b",
   "metadata": {},
   "source": [
    "Finally, we'll load our Kafka consumers and producers. As we mentioned in `Section 2` of `02-answers.md`, we've implemented *two* producers and *two* consumers:\n",
    "- One producer has been implemented using the `confluent` Python package, the other has been implemented using the `kafka-python` package.\n",
    "- One consumer has been implemented using the `confluent` Python package, the other has been implemented using the `kafka-python` package. Importantly, the `confluent` consumer can only read messages written by the `confluent` producer, and the `kafka-python` consumer can only read messages written by the `kafka-python` producer.\n",
    "\n",
    "With this in mind, let's load our two producers and two consumers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e303e84f-d383-41a2-b619-280c87eec188",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCERS = {\"confluent\": confluent_producer, \"kafka-python\": kafka_python_producer}\n",
    "CONSUMERS = {\"confluent\": confluent_consumer, \"kafka-python\": kafka_python_consumer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4239f-0aee-4bcb-ae89-aad6e718d09b",
   "metadata": {},
   "source": [
    "## Testing of Kafka Consumers and Producers Implemented in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939ee5d-5b85-4ea0-940f-c41ff0e0843b",
   "metadata": {},
   "source": [
    "Let's first do a quick a 'test run' with both of our producers. In particular, we'll write a couple of messages from each taxi CSV to a topic that are specific to each produer implementation *and* to each taxi service type (e.g. with the `confluent` producer, we'll write the green taxi trips to the `'confluent_green_trips'` topic). \n",
    "\n",
    "If this cell throws an error, then either one of two problems has occurred:\n",
    "1. The Kafka cluster is not running - make sure you've run `docker compose up -d` before running this notebook.\n",
    "1. The Kafka cluster is still starting up - wait five or so seconds before re-rerunning this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e807a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing green taxi data with Producer implemented in confluent:\n",
      "Record successfully produced to Partition 0 of topic 'confluent_green_trips' at offset 0.\n",
      "Record successfully produced to Partition 0 of topic 'confluent_green_trips' at offset 1.\n",
      "Record successfully produced to Partition 0 of topic 'confluent_green_trips' at offset 2.\n",
      "Successfully produced 3 messages to 'confluent_green_trips' topic.\n",
      "\n",
      "Producing fhv taxi data with Producer implemented in confluent:\n",
      "Record successfully produced to Partition 0 of topic 'confluent_fhv_trips' at offset 0.\n",
      "Record successfully produced to Partition 0 of topic 'confluent_fhv_trips' at offset 1.\n",
      "Record successfully produced to Partition 0 of topic 'confluent_fhv_trips' at offset 2.\n",
      "Successfully produced 3 messages to 'confluent_fhv_trips' topic.\n",
      "\n",
      "Producing green taxi data with Producer implemented in kafka-python:\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_green_trips' at offset 0.\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_green_trips' at offset 1.\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_green_trips' at offset 2.\n",
      "Successfully produced 3 messages to 'kafka-python_green_trips' topic.\n",
      "\n",
      "Producing fhv taxi data with Producer implemented in kafka-python:\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_fhv_trips' at offset 0.\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_fhv_trips' at offset 1.\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_fhv_trips' at offset 2.\n",
      "Successfully produced 3 messages to 'kafka-python_fhv_trips' topic.\n"
     ]
    }
   ],
   "source": [
    "max_messages = 3\n",
    "sleep = 0.0\n",
    "# Loop over each producer implementation:\n",
    "for i, (package_name, producer) in enumerate(PRODUCERS.items()):\n",
    "    # Loop over each taxi CSV:\n",
    "    for j, (taxi, csv_path) in enumerate(TAXI_CSVS.items()):\n",
    "        # Add space between outputs of each taxi-producer combo:\n",
    "        if i > 0 or j > 0:\n",
    "            print()\n",
    "        print(\n",
    "            f\"Producing {taxi} taxi data with Producer implemented in {package_name}:\"\n",
    "        )\n",
    "        producer.produce_taxi_data(\n",
    "            csv_path=csv_path,\n",
    "            topic=f\"{package_name}_{taxi}_trips\",\n",
    "            key_schema=SETTINGS[f\"{taxi}_key_schema\"],\n",
    "            value_schema=SETTINGS[f\"{taxi}_value_schema\"],\n",
    "            bootstrap_servers=SETTINGS[\"bootstrap_servers\"],\n",
    "            schema_registry_url=SETTINGS[\"schema_registry_url\"],\n",
    "            max_messages=max_messages,\n",
    "            sleep=sleep,\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c6e62-04ef-4d6f-936f-fd12ac8b8724",
   "metadata": {},
   "source": [
    "Let's now check that our consumers are working correctly - with each consumer, we'll read from both the FHV taxi and Green taxi topics written to by the corresponding producer (e.g. for the `kafka_python` consumer, we'll read from the ``'kafka_python_green_trips'`` and the ``'kafka_python_fhv_trips'`` topics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b18a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consuming from 'confluent_green_trips' topic with confluent consumer:\n",
      "Record 1, Key: {'lpep_pickup_datetime': '2018-12-21 15:17:29'}, Values: {'PULocationID': 264, 'lpep_pickup_datetime': '2018-12-21 15:17:29', 'lpep_dropoff_datetime': '2018-12-21 15:18:57', 'passenger_count': 5, 'trip_distance': 0.0, 'fare_amount': 3.0}\n",
      "Record 2, Key: {'lpep_pickup_datetime': '2019-01-01 00:10:16'}, Values: {'PULocationID': 97, 'lpep_pickup_datetime': '2019-01-01 00:10:16', 'lpep_dropoff_datetime': '2019-01-01 00:16:32', 'passenger_count': 2, 'trip_distance': 0.8600000143051147, 'fare_amount': 6.0}\n",
      "Record 3, Key: {'lpep_pickup_datetime': '2019-01-01 00:27:11'}, Values: {'PULocationID': 49, 'lpep_pickup_datetime': '2019-01-01 00:27:11', 'lpep_dropoff_datetime': '2019-01-01 00:31:38', 'passenger_count': 2, 'trip_distance': 0.6600000262260437, 'fare_amount': 4.5}\n",
      "Succesfully consumed 3 records from topics: ['confluent_green_trips'].\n",
      "\n",
      "Consuming from 'confluent_fhv_trips' topic with confluent consumer:\n",
      "Record 1, Key: {'pickup_datetime': '2019-01-01 00:30:00'}, Values: {'PUlocationID': None, 'DOlocationID': None, 'pickup_datetime': '2019-01-01 00:30:00', 'dropOff_datetime': '2019-01-01 02:51:55', 'dispatching_base_num': 'B00001', 'SR_Flag': None}\n",
      "Record 2, Key: {'pickup_datetime': '2019-01-01 00:45:00'}, Values: {'PUlocationID': None, 'DOlocationID': None, 'pickup_datetime': '2019-01-01 00:45:00', 'dropOff_datetime': '2019-01-01 00:54:49', 'dispatching_base_num': 'B00001', 'SR_Flag': None}\n",
      "Record 3, Key: {'pickup_datetime': '2019-01-01 00:15:00'}, Values: {'PUlocationID': None, 'DOlocationID': None, 'pickup_datetime': '2019-01-01 00:15:00', 'dropOff_datetime': '2019-01-01 00:54:52', 'dispatching_base_num': 'B00001', 'SR_Flag': None}\n",
      "Succesfully consumed 3 records from topics: ['confluent_fhv_trips'].\n",
      "\n",
      "Consuming from 'kafka-python_green_trips' topic with kafka-python consumer:\n",
      "Record 1, Key: ['2018-12-21 15:17:29'], Values: ['264', '2018-12-21 15:17:29', '2018-12-21 15:18:57', '5', '.00', '3']\n",
      "Record 2, Key: ['2019-01-01 00:10:16'], Values: ['97', '2019-01-01 00:10:16', '2019-01-01 00:16:32', '2', '.86', '6']\n",
      "Record 3, Key: ['2019-01-01 00:27:11'], Values: ['49', '2019-01-01 00:27:11', '2019-01-01 00:31:38', '2', '.66', '4.5']\n",
      "Succesfully consumed 3 records from topics: ['kafka-python_green_trips'].\n",
      "\n",
      "Consuming from 'kafka-python_fhv_trips' topic with kafka-python consumer:\n",
      "Record 1, Key: ['2019-01-01 00:30:00'], Values: ['', '', '2019-01-01 00:30:00', '2019-01-01 02:51:55', 'B00001', '']\n",
      "Record 2, Key: ['2019-01-01 00:45:00'], Values: ['', '', '2019-01-01 00:45:00', '2019-01-01 00:54:49', 'B00001', '']\n",
      "Record 3, Key: ['2019-01-01 00:15:00'], Values: ['', '', '2019-01-01 00:15:00', '2019-01-01 00:54:52', 'B00001', '']\n",
      "Succesfully consumed 3 records from topics: ['kafka-python_fhv_trips'].\n"
     ]
    }
   ],
   "source": [
    "min_messages = 3\n",
    "sleep = 0.0\n",
    "for i, (package_name, consumer) in enumerate(CONSUMERS.items()):\n",
    "    for j, (taxi, csv_path) in enumerate(TAXI_CSVS.items()):\n",
    "        if i > 0 or j > 0:\n",
    "            print()\n",
    "        topic = f\"{package_name}_{taxi}_trips\"\n",
    "        print(f\"Consuming from '{topic}' topic with {package_name} consumer:\")\n",
    "        consumer.consume_taxi_data(\n",
    "            topic=topic,\n",
    "            key_schema=SETTINGS[f\"{taxi}_key_schema\"],\n",
    "            value_schema=SETTINGS[f\"{taxi}_value_schema\"],\n",
    "            schema_registry_url=SETTINGS[\"schema_registry_url\"],\n",
    "            bootstrap_servers=SETTINGS[\"bootstrap_servers\"],\n",
    "            group_id=f\"{package_name}_{taxi}_taxi\",\n",
    "            offset=\"earliest\",\n",
    "            min_messages=min_messages,\n",
    "            sleep=sleep,\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1f791-f9f0-4042-94ad-6ac41a079ac9",
   "metadata": {},
   "source": [
    "Pleasingly, it appears that our consumers have correctly read the records written by our producers. Now that we know that our Kafka consumers and producers have been implemented correctly, let's now move on to actually analysing the data in these topics using `pyspark`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf379ba-bce4-4f12-8ae1-c921a47d9f3d",
   "metadata": {},
   "source": [
    "## Connecting PySpark to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e2618-d82b-4ff0-9b8b-6525005e0833",
   "metadata": {},
   "source": [
    "We'll first need to start our Spark session. In order to have PySpark correctly interact with our Kafka cluster, we'll have to correctly specify the `PYSPARK_SUBMIT_ARGS` environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5287376-b64b-489c-b7ee-26eb94110aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,\"\n",
    "    \"org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell\"\n",
    ")\n",
    "spark = SparkSession.builder.appName(\"kafka-example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def36f71",
   "metadata": {},
   "source": [
    "PySpark - we should make sure to clear this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1ef7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"checkpoint\"):\n",
    "    shutil.rmtree(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c45d8",
   "metadata": {},
   "source": [
    "For the sake of reproducibility, we'll also take this opportunity to print out the version of Spark we're using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3bd347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/27 23:32:10 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.1.146 instead (on interface enp9s0)\n",
      "23/03/27 23:32:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.17\n",
      "Branch HEAD\n",
      "Compiled by user liangchi on 2023-02-10T19:57:40Z\n",
      "Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675d66ea-20cc-4fee-9574-bc89c7b16fdb",
   "metadata": {},
   "source": [
    "For the purposes of writing data to our Kafka cluster that we'll analyse using PySpark, we'll restrict ourself to **only using the `python-kafka` producer**. We choose the `python_kafka` producer over the `confluent` producer here since the it's a lot easier to deserialize the messages written by with `python-kafka` (which are just plain strings that contain all the message values concatenated together) than those written by with `confluent`.\n",
    "\n",
    "With this in mind, we'll define the following function to write a specified number of rows in the FHV taxi and Green taxi CSVs to the topics `fhv_trips` and `green_trips` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5704ad8-1503-43e3-959d-3a818f133d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_taxi_data_with_kafka_python(num_messages: int) -> None:\n",
    "    \"\"\"\n",
    "    Using `kafka_python_producer`, writes the first `num_messages`\n",
    "    rows of the Green taxi CSV data and the first `num_messages` rows\n",
    "    of the FHV taxi CSV data to the 'green_trips' and `fhv_trips`\n",
    "    topics respectively.\n",
    "    \"\"\"\n",
    "    for taxi, csv_path in TAXI_CSVS.items():\n",
    "        kafka_python_producer.produce_taxi_data(\n",
    "            csv_path=csv_path,\n",
    "            topic=f\"{taxi}_trips\",\n",
    "            key_schema=SETTINGS[f\"{taxi}_key_schema\"],\n",
    "            value_schema=SETTINGS[f\"{taxi}_value_schema\"],\n",
    "            bootstrap_servers=SETTINGS[\"bootstrap_servers\"],\n",
    "            schema_registry_url=SETTINGS[\"schema_registry_url\"],\n",
    "            max_messages=num_messages,\n",
    "            sleep=0.0,\n",
    "            verbose=False,\n",
    "        )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1063e",
   "metadata": {},
   "source": [
    "Let's now use our function to write to twenty thousand messages to each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dda9a83-58b1-4bb4-a6db-f7125029e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully produced 20000 messages to 'green_trips' topic.\n",
      "Successfully produced 20000 messages to 'fhv_trips' topic.\n"
     ]
    }
   ],
   "source": [
    "produce_taxi_data_with_kafka_python(num_messages=20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e373e80-ca8c-40a3-8c63-516001bd76a3",
   "metadata": {},
   "source": [
    "### Loading Kafka Messages into PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3af1f-6bf3-4f2a-8070-831f5a9e41bb",
   "metadata": {},
   "source": [
    "Now that we've written messages to the `green_trips` and `fhv_trips` topics, let's read from these topics using PySpark to create two streaming dataframes (i.e. one for the `green_trips` topic and another for the `fhv_trips` topic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b1f63d-bbea-4d07-bc42-22e333c713d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_raw = read_df_from_kafka(spark, topic=\"green_trips\")\n",
    "df_fhv_raw = read_df_from_kafka(spark, topic=\"fhv_trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a7737",
   "metadata": {},
   "source": [
    "To better understand what `read_df_from_kafka` has done ehre, please refer to `streaming.py`. An important point to note here is that the schema of the dataframes we've loaded here is **not** the same as the schema of the taxi trip CSVs. Instead, the schema of `df_green_raw` and `df_fhv_raw` contains the *serialized* `key` and `value` of each message we've read:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c733213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|                 key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+--------------------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|[32 30 31 38 2D 3...|[32 36 34 2C 20 3...|green_trips|        0|     0|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[39 37 2C 20 32 3...|green_trips|        0|     1|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[34 39 2C 20 32 3...|green_trips|        0|     2|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[31 38 39 2C 20 3...|green_trips|        0|     3|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[38 32 2C 20 32 3...|green_trips|        0|     4|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[34 39 2C 20 32 3...|green_trips|        0|     5|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[32 35 35 2C 20 3...|green_trips|        0|     6|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[37 36 2C 20 32 3...|green_trips|        0|     7|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[32 35 2C 20 32 3...|green_trips|        0|     8|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[38 35 2C 20 32 3...|green_trips|        0|     9|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[32 32 33 2C 20 3...|green_trips|        0|    10|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[31 32 39 2C 20 3...|green_trips|        0|    11|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[37 31 2C 20 32 3...|green_trips|        0|    12|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[38 35 2C 20 32 3...|green_trips|        0|    13|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[32 35 36 2C 20 3...|green_trips|        0|    14|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[38 30 2C 20 32 3...|green_trips|        0|    15|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[32 35 36 2C 20 3...|green_trips|        0|    16|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[32 35 35 2C 20 3...|green_trips|        0|    17|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 38 2D 3...|[31 34 36 2C 20 3...|green_trips|        0|    18|2023-03-27 23:32:...|            0|\n",
      "|[32 30 31 39 2D 3...|[31 34 36 2C 20 3...|green_trips|        0|    19|2023-03-27 23:32:...|            0|\n",
      "+--------------------+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_streaming_df(df_green_raw, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39918f64",
   "metadata": {},
   "source": [
    "Before actually working with the data we've loaded from each topic then, we'll have to deserialize the `value` of each loaded message. To do this, first need to declare to Pyspark the schema of the message values we're going to load, which we already know ahead of time (see Arvo files in `schema/` directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "906608b3-f953-4486-a41b-b186e85a40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"PULocationID\", T.IntegerType()),\n",
    "        T.StructField(\"lpep_pickup_datetime\", T.TimestampType()),\n",
    "        T.StructField(\"lpep_dropoff_datetime\", T.TimestampType()),\n",
    "        T.StructField(\"passenger_count\", T.IntegerType()),\n",
    "        T.StructField(\"trip_distance\", T.FloatType()),\n",
    "        T.StructField(\"fare_amount\", T.FloatType()),\n",
    "    ]\n",
    ")\n",
    "fhv_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"PUlocationID\", T.IntegerType()),\n",
    "        T.StructField(\"DOlocationID\", T.IntegerType()),\n",
    "        T.StructField(\"pickup_datetime\", T.TimestampType()),\n",
    "        T.StructField(\"dropOff_datetime\", T.TimestampType()),\n",
    "        T.StructField(\"dispatching_base_num\", T.StringType()),\n",
    "        T.StructField(\"SR_Flag\", T.BooleanType()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4ec6b",
   "metadata": {},
   "source": [
    "With the schema of our messages defined, we can deserialize and parse each message value using the `parse_kafka_df` function we've defined in `streaming.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b177ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = parse_taxi_messages(df_green_raw, green_schema)\n",
    "df_fhv = parse_taxi_messages(df_fhv_raw, fhv_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1be1f2",
   "metadata": {},
   "source": [
    "To confirm that we've correctly parsed the Kafka messages we've read, let's now print the contents of the `df_green` and `df_fhv` dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0dc714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Green taxi dataframe:\n",
      "+------------+--------------------+---------------------+---------------+-------------+-----------+\n",
      "|PULocationID|lpep_pickup_datetime|lpep_dropoff_datetime|passenger_count|trip_distance|fare_amount|\n",
      "+------------+--------------------+---------------------+---------------+-------------+-----------+\n",
      "|         264| 2018-12-21 15:17:29|  2018-12-21 15:18:57|              5|          0.0|        3.0|\n",
      "|          97| 2019-01-01 00:10:16|  2019-01-01 00:16:32|              2|         0.86|        6.0|\n",
      "|          49| 2019-01-01 00:27:11|  2019-01-01 00:31:38|              2|         0.66|        4.5|\n",
      "|         189| 2019-01-01 00:46:20|  2019-01-01 01:04:54|              2|         2.68|       13.5|\n",
      "|          82| 2019-01-01 00:19:06|  2019-01-01 00:39:43|              1|         4.53|       18.0|\n",
      "|          49| 2019-01-01 00:12:35|  2019-01-01 00:19:09|              1|         1.05|        6.5|\n",
      "|         255| 2019-01-01 00:47:55|  2019-01-01 01:00:01|              1|         3.77|       13.5|\n",
      "|          76| 2019-01-01 00:12:47|  2019-01-01 00:30:50|              1|          4.1|       16.0|\n",
      "|          25| 2019-01-01 00:16:23|  2019-01-01 00:39:46|              1|         7.75|       25.5|\n",
      "|          85| 2019-01-01 00:58:02|  2019-01-01 01:19:02|              1|         3.68|       15.5|\n",
      "|         223| 2019-01-01 00:37:00|  2019-01-01 00:56:42|              1|         6.84|       22.0|\n",
      "|         129| 2019-01-01 00:13:48|  2019-01-01 00:21:00|              2|         1.15|        6.5|\n",
      "|          71| 2019-01-01 00:19:59|  2019-01-01 00:45:50|              1|         0.49|       15.5|\n",
      "|          85| 2019-01-01 00:57:57|  2019-01-01 01:20:10|              1|         3.61|       17.0|\n",
      "|         256| 2019-01-01 00:09:02|  2019-01-01 00:17:50|              1|          1.2|        7.5|\n",
      "|          80| 2019-01-01 00:22:12|  2019-01-01 00:25:29|              1|          0.5|        4.0|\n",
      "|         256| 2019-01-01 00:31:55|  2019-01-01 00:52:59|              1|          5.5|       19.5|\n",
      "|         255| 2019-01-01 00:30:20|  2019-01-01 00:54:19|              1|         5.01|       20.0|\n",
      "|         146| 2018-12-31 23:58:06|  2019-01-01 00:00:57|              1|         0.43|        4.0|\n",
      "|         146| 2019-01-01 00:40:17|  2019-01-01 00:50:23|              1|         2.72|       10.5|\n",
      "+------------+--------------------+---------------------+---------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Parsed FHV taxi dataframe:\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+\n",
      "|PUlocationID|DOlocationID|    pickup_datetime|   dropOff_datetime|dispatching_base_num|SR_Flag|\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+\n",
      "|        null|        null|2019-01-01 00:30:00|2019-01-01 02:51:55|              B00001|   null|\n",
      "|        null|        null|2019-01-01 00:45:00|2019-01-01 00:54:49|              B00001|   null|\n",
      "|        null|        null|2019-01-01 00:15:00|2019-01-01 00:54:52|              B00001|   null|\n",
      "|        null|        null|2019-01-01 00:19:00|2019-01-01 00:39:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:27:00|2019-01-01 00:37:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:48:00|2019-01-01 01:02:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:50:00|2019-01-01 00:59:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:51:00|2019-01-01 00:56:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:44:00|2019-01-01 00:58:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:19:00|2019-01-01 00:36:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:36:00|2019-01-01 00:49:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:26:00|2019-01-01 00:32:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:26:00|2019-01-01 00:36:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:58:00|2019-01-01 01:05:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:02:29|2019-01-02 00:25:30|              B00013|   null|\n",
      "|        null|        null|2019-01-01 00:01:33|2019-01-02 00:18:16|              B00013|   null|\n",
      "|        null|         265|2019-01-01 00:02:43|2019-01-01 00:10:14|              B00037|   null|\n",
      "|        null|         265|2019-01-01 00:26:02|2019-01-01 00:37:00|              B00037|   null|\n",
      "|        null|         265|2019-01-01 00:11:16|2019-01-01 00:25:41|              B00037|   null|\n",
      "|        null|         265|2019-01-01 00:33:45|2019-01-01 00:45:28|              B00037|   null|\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsed Green taxi dataframe:\")\n",
    "show_streaming_df(df_green, spark)\n",
    "print(\"Parsed FHV taxi dataframe:\")\n",
    "show_streaming_df(df_fhv, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80edca55-5607-4f69-ad07-b758800573d0",
   "metadata": {},
   "source": [
    "### Combining Streaming Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb0fe0-d9d3-4a92-a713-44b6e7cf2e0f",
   "metadata": {},
   "source": [
    "Now that we have our `df_green` and `df_fhv` streaming dataframes, let's now merge these two streaming dataframes into a unified `df_rides` dataframe. To do achieve this, let's first ensure that the names of the columns shared by `df_green` and `df_fhv` have the same name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94cf79b2-b9dd-4a54-ba45-29f45120e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now \"pickup_datetime\" and \"dropoff_datetime\" in both `df_green` and `df_fhv`:\n",
    "df_green = df_green.withColumnRenamed(\n",
    "    \"lpep_pickup_datetime\", \"pickup_datetime\"\n",
    ").withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "df_fhv = df_fhv.withColumnRenamed(\"PUlocationID\", \"PULocationID\")\n",
    "# Now \"PULocationID\" in both `df_green` and `df_fhv`:\n",
    "df_fhv = df_fhv.withColumnRenamed(\"PUlocationID\", \"PULocationID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26726ce6-590e-4044-bab9-64990adf111f",
   "metadata": {},
   "source": [
    "We'll also add a `service_type` column to `df_green` and `df_fhv` so that once we merge these dataframes into a single dataframe, we'll be able to track whether a record came from the Green taxi dataset or the FHV taxi dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1e48514-2aa8-4277-ad73-fb92fe774a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green.withColumn(\"service_type\", F.lit(\"green\").cast(\"string\"))\n",
    "df_fhv = df_fhv.withColumn(\"service_type\", F.lit(\"fhv\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667d8aa-a18e-4b04-b6d5-f62531c0d3d6",
   "metadata": {},
   "source": [
    "The final step we need to perform before we merge our two dataframes is to add a timestamp column to each dataframe, and then set this column as a *watermark*. Without going into detail here, watermarks are used PySpark to correctly handle messages that 'arrive late'. For more information on how watermarks work, please refer to [this blog post](https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html), as well as [this blog post](https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e0412b9-e331-4ede-a152-45431b384c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_wm = df_fhv.withColumn(\"timestamp\", F.current_timestamp()).withWatermark(\n",
    "    \"timestamp\", \"1 hours\"\n",
    ")\n",
    "df_green_wm = df_green.withColumn(\"timestamp\", F.current_timestamp()).withWatermark(\n",
    "    \"timestamp\", \"1 hours\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef92cb",
   "metadata": {},
   "source": [
    "We're now able to unify our two streaming dataframes into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5093b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rides = df_fhv_wm.unionByName(df_green_wm, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9287f8a",
   "metadata": {},
   "source": [
    "Let's print our newly created `df_rides` dataframes to make sure this unification has ocurred correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc3dd732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------------+-------------------+--------------------+-------+------------+--------------------+---------------+-------------+-----------+\n",
      "|PULocationID|DOlocationID|    pickup_datetime|   dropOff_datetime|dispatching_base_num|SR_Flag|service_type|           timestamp|passenger_count|trip_distance|fare_amount|\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+------------+--------------------+---------------+-------------+-----------+\n",
      "|        null|        null|2019-01-01 00:30:00|2019-01-01 02:51:55|              B00001|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:45:00|2019-01-01 00:54:49|              B00001|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:15:00|2019-01-01 00:54:52|              B00001|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:19:00|2019-01-01 00:39:00|              B00008|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:27:00|2019-01-01 00:37:00|              B00008|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:48:00|2019-01-01 01:02:00|              B00008|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:50:00|2019-01-01 00:59:00|              B00008|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:51:00|2019-01-01 00:56:00|              B00008|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:44:00|2019-01-01 00:58:00|              B00009|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:19:00|2019-01-01 00:36:00|              B00009|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:36:00|2019-01-01 00:49:00|              B00009|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:26:00|2019-01-01 00:32:00|              B00009|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:26:00|2019-01-01 00:36:00|              B00009|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:58:00|2019-01-01 01:05:00|              B00009|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:02:29|2019-01-02 00:25:30|              B00013|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:01:33|2019-01-02 00:18:16|              B00013|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|         265|2019-01-01 00:02:43|2019-01-01 00:10:14|              B00037|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|         265|2019-01-01 00:26:02|2019-01-01 00:37:00|              B00037|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|         265|2019-01-01 00:11:16|2019-01-01 00:25:41|              B00037|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "|        null|         265|2019-01-01 00:33:45|2019-01-01 00:45:28|              B00037|   null|         fhv|2023-03-27 23:32:...|           null|         null|       null|\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+------------+--------------------+---------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_streaming_df(df_rides, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97716cb-6d49-48e2-bd01-efd2a1e525d5",
   "metadata": {},
   "source": [
    "### Querying Streaming Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e7ecb",
   "metadata": {},
   "source": [
    "Now that we've combined the Green and FHV taxi trips into a single streaming dataframe, let's try to work out the ID of the most popular pickup location across all of these taxi trips. \n",
    "\n",
    "For this purpose, we'll define the function `count_pu_locations`, which will use a simple SQL query to determine the top `num_result` most popular pick-up location IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0abbf3a-5f0e-467c-9976-c47d8e2cc9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pu_locations(df, spark, num_result, query_name=\"count_pu_locs\"):\n",
    "    query = (\n",
    "        df.writeStream.queryName(query_name)\n",
    "        .format(\"memory\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    "    )\n",
    "    query.awaitTermination()\n",
    "    query_results = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            PULocationID, COUNT(PULocationID) AS count\n",
    "        FROM\n",
    "            {query_name}\n",
    "        GROUP BY\n",
    "            PULocationID\n",
    "        ORDER BY\n",
    "            count DESC\n",
    "        LIMIT {num_result};\n",
    "    \"\"\"\n",
    "    )\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d94f9a",
   "metadata": {},
   "source": [
    "Using `count_pu_locations`, let's find out the top 10 most popular pick-up location IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30b65ecf-05ec-4199-8055-cd995a9cb638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PULocationID|count|\n",
      "+------------+-----+\n",
      "|          74| 1623|\n",
      "|          41| 1220|\n",
      "|           7| 1219|\n",
      "|         181|  983|\n",
      "|          42|  961|\n",
      "|          75|  912|\n",
      "|         129|  859|\n",
      "|          82|  855|\n",
      "|         255|  848|\n",
      "|          97|  698|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_pu_locations(df_rides, spark, num_result=10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ea5d0",
   "metadata": {},
   "source": [
    "The convenient thing about *streaming* dataframes is that they *automatically update* when more data is produced to a topic. To illustrate this, let's write some more data to the `fhv_trips` and `green_trips` topics and then recompute the top 10 most popular pickup location IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ec29207-1cc5-42a2-ad01-abb14a49d342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add new data to Kafka topics:\n",
      "Successfully produced 500 messages to 'green_trips' topic.\n",
      "Successfully produced 500 messages to 'fhv_trips' topic.\n",
      "Re-run count:\n",
      "+------------+-----+\n",
      "|PULocationID|count|\n",
      "+------------+-----+\n",
      "|          74| 1641|\n",
      "|          41| 1250|\n",
      "|           7| 1246|\n",
      "|         181| 1016|\n",
      "|          42|  985|\n",
      "|          75|  929|\n",
      "|         255|  872|\n",
      "|         129|  871|\n",
      "|          82|  870|\n",
      "|          97|  713|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Add new data to Kafka topics:\")\n",
    "produce_taxi_data_with_kafka_python(num_messages=500)\n",
    "print(\"Re-run count:\")\n",
    "count_pu_locations(df_rides, spark, num_result=10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc1485",
   "metadata": {},
   "source": [
    "As expected, the result of our query has changed upon writing more data to the topics we're streaming data from - in particular, check the values in the `count` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079b8c4-c840-4c8a-98ca-fdb17fe07470",
   "metadata": {},
   "source": [
    "### Writing Straming Dataframes to a Kafka Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fce90",
   "metadata": {},
   "source": [
    "Now that we've queried our combined `df_trips` dataframe, let's now try to write this dataframe to a new topic in our Kafka cluster.\n",
    "\n",
    "Unfortunately, we can't directly write our `df_trips` in its current form to Kafka; this is because Kafka is expecting a dataframe with a `key` column, which stores the *serialized* key of the message we're writing, and a `value` column, which stores the *serialized* values of the message we're writing. In this case, our serialized messages values are simply a serialized strings, where each string is formed by joining together the values we want to store together with commas.\n",
    "\n",
    "With this in mind, we'll prepare our `df_rides` dataframe for writing by using the `prepare_df_for_producing` function we've defined in `streaming.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28b1dd63-0000-413e-b459-cdac78f1e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rides_produce = prepare_df_for_producing(\n",
    "    df_rides,\n",
    "    value_columns=[\"service_type\", \"pickup_datetime\", \"dropoff_datetime\"],\n",
    "    key_column=\"pickup_datetime\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa1ba38",
   "metadata": {},
   "source": [
    "Now that we've defined `df_rides_produce` (which is just `df_trips` put into a form that is suitable for writing to a Kafka topic), we can now use the `write_df_to_topic` function we've defined in `streaming.py` to continuously write the contents of this dataframe to a new topic, which we'll call `all_trips`. \n",
    "\n",
    "Since `df_rides_produce` is a streaming dataframe whose contents will automatically update should new data be written to the `green_trips` and/or `fhv_trips` topics, we also need to specify a writing frequency to `write_df_to_topic`: this is the number of seconds that PySpark should periodically wait before checking if any new data has been added to `df_rides_produce`. If any new data has been written to this dataframe since the last time PySpark checked, the new data will be automatically appended to the specified topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b998080-0043-4c35-8bef-3042ec78fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_freq = 1\n",
    "write_query = write_df_to_topic(\n",
    "    df_rides_produce, topic=\"all_trips\", write_freq=write_freq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13371d2-a0fe-4f03-a0f8-a539ecde2c1f",
   "metadata": {},
   "source": [
    "To make sure that we've successfully written all the messages currently stored in the `green_trips` and `fhv_trips` topics (i.e. the Kafka topics that the `df_rides_produce` dataframe is streaming from) to the new `all_trips` topic, let's read a dataframe from the `all_trips` topic we've just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65e7b3f9-5d64-49a8-9988-dd88b2a77329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+---------+------+--------------------+-------------+\n",
      "|                 key|               value|    topic|partition|offset|           timestamp|timestampType|\n",
      "+--------------------+--------------------+---------+---------+------+--------------------+-------------+\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|     0|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 38 2D 3...|[67 72 65 65 6E 2...|all_trips|        0|     1|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|     2|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[67 72 65 65 6E 2...|all_trips|        0|     3|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|     4|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|     5|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[67 72 65 65 6E 2...|all_trips|        0|     6|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|     7|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[67 72 65 65 6E 2...|all_trips|        0|     8|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|     9|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|    10|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[67 72 65 65 6E 2...|all_trips|        0|    11|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|    12|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[67 72 65 65 6E 2...|all_trips|        0|    13|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|    14|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|    15|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[67 72 65 65 6E 2...|all_trips|        0|    16|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|    17|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[67 72 65 65 6E 2...|all_trips|        0|    18|2023-03-27 23:33:...|            0|\n",
      "|[32 30 31 39 2D 3...|[66 68 76 2C 20 3...|all_trips|        0|    19|2023-03-27 23:33:...|            0|\n",
      "+--------------------+--------------------+---------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rides_read = read_df_from_kafka(spark, topic=\"all_trips\")\n",
    "# Wait until all new data has streamed into new topic:\n",
    "while write_query.status[\"isTriggerActive\"]:\n",
    "    time.sleep(0.1)\n",
    "show_streaming_df(df_rides_read, spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86cc9b-caf3-46f9-b65c-84f10d96ce6a",
   "metadata": {},
   "source": [
    "Let's also count the number of records we've written to the `all_trips` topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86979ddb-29e3-4537-bbb5-3f8c8e83b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in `all_trips`: 41000\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Number of records in `all_trips`: {count_streaming_df_rows(df_rides_read, spark)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b775bc-b3cb-461e-9ad6-1f8d08fb96ca",
   "metadata": {},
   "source": [
    "Finally, let's check that any new data that's written to the `green_trips` and `fhv_trips` topic is *automatically* streamed to the `df_trips` and `df_trips_produce` dataframes, and then is automatically written to the `all_trips` topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4371d25c-1c66-40df-aa9b-c1bedfc19fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully produced 123 messages to 'green_trips' topic.\n",
      "Successfully produced 123 messages to 'fhv_trips' topic.\n",
      "Number of rows: 41246\n"
     ]
    }
   ],
   "source": [
    "# Write some more messages to `green_trips` and `fhv_trips`:\n",
    "produce_taxi_data_with_kafka_python(num_messages=123)\n",
    "# Wait for PySpark to check for new messages written to DataFrame:\n",
    "time.sleep(1.5 * write_freq)\n",
    "# Wait until all new data has streamed into new topic:\n",
    "while write_query.status[\"isTriggerActive\"]:\n",
    "    time.sleep(0.1)\n",
    "print(f\"Number of rows: {count_streaming_df_rows(df_rides_read, spark)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f4d65-be0e-42e0-8366-47f722fc6cdf",
   "metadata": {},
   "source": [
    "Pleasingly, we can see that PySpark has automatically written the messages we sent to `green_trips` and `fhv_trips` to the `all_trips` topic.\n",
    "\n",
    "If we no longer want PySpark to automatically update the `all_trips` topic with new observations in `df_rides_produce`, we just need to call the `stop()` method of the `write_query` object returned by our `write_df_to_topic` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91bda402-08e9-4f76-a902-dfa5d599ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560d806-1840-463d-af3b-bf10bd941d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
