{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c8c668-50e1-4278-8c7c-75085c6cde30",
   "metadata": {},
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff355ae1-4aa2-4ba5-aaae-6dc6944ab50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from producers import confluent_producer, kafka_python_producer\n",
    "from consumers import confluent_consumer, kafka_python_consumer\n",
    "from utils import download_csv, load_kafka_settings\n",
    "from streaming import (\n",
    "    parse_ride_df_from_kafka,\n",
    "    write_batch_df_to_kafka,\n",
    "    write_streaming_df_to_kafka,\n",
    "    load_df_from_kafka,\n",
    "    streaming_df_to_batch_df,\n",
    "    prepare_df_for_kafka,\n",
    "    streaming_df_row_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e303e84f-d383-41a2-b619-280c87eec188",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCERS = {\"confluent\": confluent_producer, \"kafka-python\": kafka_python_producer}\n",
    "CONSUMERS = {\"confluent\": confluent_consumer, \"kafka-python\": kafka_python_consumer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b5e68b-f179-446a-97d8-b0e4c957fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = load_kafka_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52169fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_csv_path = download_csv(\n",
    "    \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-01.csv.gz\"\n",
    ")\n",
    "fhv_csv_path = download_csv(\n",
    "    \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-01.csv.gz\"\n",
    ")\n",
    "TAXI_CSVS = {\"green\": green_csv_path, \"fhv\": fhv_csv_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4239f-0aee-4bcb-ae89-aad6e718d09b",
   "metadata": {},
   "source": [
    "## Testing of Kafka Consumers and Producers Implemented in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e807a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing green taxi data with Producer implemented in confluent:\n",
      "Record successfully produced to Partition 0 of topic 'confluent_green_trips' at offset 3.\n",
      "Record successfully produced to Partition 0 of topic 'confluent_green_trips' at offset 4.\n",
      "Record successfully produced to Partition 0 of topic 'confluent_green_trips' at offset 5.\n",
      "Successfully produced 3 messages to 'confluent_green_trips' topic.\n",
      "\n",
      "Producing fhv taxi data with Producer implemented in confluent:\n",
      "Record successfully produced to Partition 0 of topic 'confluent_fhv_trips' at offset 3.\n",
      "Record successfully produced to Partition 0 of topic 'confluent_fhv_trips' at offset 4.\n",
      "Record successfully produced to Partition 0 of topic 'confluent_fhv_trips' at offset 5.\n",
      "Successfully produced 3 messages to 'confluent_fhv_trips' topic.\n",
      "\n",
      "Producing green taxi data with Producer implemented in kafka-python:\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_green_trips' at offset 3.\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_green_trips' at offset 4.\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_green_trips' at offset 5.\n",
      "Successfully produced 3 messages to 'kafka-python_green_trips' topic.\n",
      "\n",
      "Producing fhv taxi data with Producer implemented in kafka-python:\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_fhv_trips' at offset 3.\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_fhv_trips' at offset 4.\n",
      "Record successfully produced to Partition 0 of topic 'kafka-python_fhv_trips' at offset 5.\n",
      "Successfully produced 3 messages to 'kafka-python_fhv_trips' topic.\n"
     ]
    }
   ],
   "source": [
    "max_messages = 3\n",
    "sleep = 0.0\n",
    "for i, (package_name, producer) in enumerate(PRODUCERS.items()):\n",
    "    for j, (taxi, csv_path) in enumerate(TAXI_CSVS.items()):\n",
    "        if i > 0 or j > 0:\n",
    "            print()\n",
    "        print(\n",
    "            f\"Producing {taxi} taxi data with Producer implemented in {package_name}:\"\n",
    "        )\n",
    "        producer.produce_taxi_data(\n",
    "            csv_path=csv_path,\n",
    "            topic=f\"{package_name}_{taxi}_trips\",\n",
    "            key_schema=SETTINGS[f\"{taxi}_key_schema\"],\n",
    "            value_schema=SETTINGS[f\"{taxi}_value_schema\"],\n",
    "            bootstrap_servers=SETTINGS[\"bootstrap_servers\"],\n",
    "            registry_url=SETTINGS[\"registry_url\"],\n",
    "            max_messages=max_messages,\n",
    "            sleep=sleep,\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b18a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consuming green taxi data with Consumer implemented in confluent:\n",
      "Record 1, Key: {'VendorID': 2}, Values: {'PULocationID': 264, 'lpep_pickup_datetime': '2018-12-21 15:17:29', 'lpep_dropoff_datetime': '2018-12-21 15:18:57', 'passenger_count': 5, 'trip_distance': 0.0, 'fare_amount': 3.0}\n",
      "Record 2, Key: {'VendorID': 2}, Values: {'PULocationID': 97, 'lpep_pickup_datetime': '2019-01-01 00:10:16', 'lpep_dropoff_datetime': '2019-01-01 00:16:32', 'passenger_count': 2, 'trip_distance': 0.8600000143051147, 'fare_amount': 6.0}\n",
      "Record 3, Key: {'VendorID': 2}, Values: {'PULocationID': 49, 'lpep_pickup_datetime': '2019-01-01 00:27:11', 'lpep_dropoff_datetime': '2019-01-01 00:31:38', 'passenger_count': 2, 'trip_distance': 0.6600000262260437, 'fare_amount': 4.5}\n",
      "Succesfully consumed 3 records from topics: ['confluent_green_trips'].\n",
      "\n",
      "Consuming fhv taxi data with Consumer implemented in confluent:\n",
      "Record 1, Key: {'DOlocationID': None}, Values: {'PUlocationID': None, 'DOlocationID': None, 'pickup_datetime': '2019-01-01 00:30:00', 'dropOff_datetime': '2019-01-01 02:51:55', 'dispatching_base_num': 'B00001', 'SR_Flag': None}\n",
      "Record 2, Key: {'DOlocationID': None}, Values: {'PUlocationID': None, 'DOlocationID': None, 'pickup_datetime': '2019-01-01 00:45:00', 'dropOff_datetime': '2019-01-01 00:54:49', 'dispatching_base_num': 'B00001', 'SR_Flag': None}\n",
      "Record 3, Key: {'DOlocationID': None}, Values: {'PUlocationID': None, 'DOlocationID': None, 'pickup_datetime': '2019-01-01 00:15:00', 'dropOff_datetime': '2019-01-01 00:54:52', 'dispatching_base_num': 'B00001', 'SR_Flag': None}\n",
      "Succesfully consumed 3 records from topics: ['confluent_fhv_trips'].\n",
      "\n",
      "Consuming green taxi data with Consumer implemented in kafka-python:\n",
      "Record 1, Key: ['2'], Values: ['264', '2018-12-21 15:17:29', '2018-12-21 15:18:57', '5', '.00', '3']\n",
      "Record 2, Key: ['2'], Values: ['97', '2019-01-01 00:10:16', '2019-01-01 00:16:32', '2', '.86', '6']\n",
      "Record 3, Key: ['2'], Values: ['49', '2019-01-01 00:27:11', '2019-01-01 00:31:38', '2', '.66', '4.5']\n",
      "Succesfully consumed 3 records from topics: ['kafka-python_green_trips'].\n",
      "\n",
      "Consuming fhv taxi data with Consumer implemented in kafka-python:\n",
      "Record 1, Key: [''], Values: ['', '', '2019-01-01 00:30:00', '2019-01-01 02:51:55', 'B00001', '']\n",
      "Record 2, Key: [''], Values: ['', '', '2019-01-01 00:45:00', '2019-01-01 00:54:49', 'B00001', '']\n",
      "Record 3, Key: [''], Values: ['', '', '2019-01-01 00:15:00', '2019-01-01 00:54:52', 'B00001', '']\n",
      "Succesfully consumed 3 records from topics: ['kafka-python_fhv_trips'].\n"
     ]
    }
   ],
   "source": [
    "taxi_types = [\"green\", \"fhv\"]\n",
    "taxi_csvs = [green_csv_path, fhv_csv_path]\n",
    "max_messages = 3\n",
    "sleep = 0.0\n",
    "for i, (package_name, consumer) in enumerate(CONSUMERS.items()):\n",
    "    for j, (taxi, csv_path) in enumerate(TAXI_CSVS.items()):\n",
    "        if i > 0 or j > 0:\n",
    "            print()\n",
    "        print(\n",
    "            f\"Consuming {taxi} taxi data with Consumer implemented in {package_name}:\"\n",
    "        )\n",
    "        consumer.consume_taxi_data(\n",
    "            topic=f\"{package_name}_{taxi}_trips\",\n",
    "            key_schema=SETTINGS[f\"{taxi}_key_schema\"],\n",
    "            value_schema=SETTINGS[f\"{taxi}_value_schema\"],\n",
    "            registry_url=SETTINGS[\"registry_url\"],\n",
    "            bootstrap_servers=SETTINGS[\"bootstrap_servers\"],\n",
    "            group_id=f\"{package_name}_{taxi}_taxi\",\n",
    "            offset=\"earliest\",\n",
    "            max_messages=max_messages,\n",
    "            sleep=sleep,\n",
    "            verbose=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf379ba-bce4-4f12-8ae1-c921a47d9f3d",
   "metadata": {},
   "source": [
    "## Connecting PySpark to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5287376-b64b-489c-b7ee-26eb94110aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,\"\n",
    "    \"org.apache.spark:spark-avro_2.12:3.3.1 pyspark-shell\"\n",
    ")\n",
    "spark = SparkSession.builder.appName(\"kafka-example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5704ad8-1503-43e3-959d-3a818f133d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_taxi_data_with_kafka_python(num_messages):\n",
    "    for taxi, csv_path in TAXI_CSVS.items():\n",
    "        kafka_python_producer.produce_taxi_data(\n",
    "            csv_path=csv_path,\n",
    "            topic=f\"{taxi}_trips\",\n",
    "            key_schema=SETTINGS[f\"{taxi}_key_schema\"],\n",
    "            value_schema=SETTINGS[f\"{taxi}_value_schema\"],\n",
    "            bootstrap_servers=SETTINGS[\"bootstrap_servers\"],\n",
    "            registry_url=SETTINGS[\"registry_url\"],\n",
    "            max_messages=num_messages,\n",
    "            sleep=0.0,\n",
    "            verbose=False,\n",
    "        )\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dda9a83-58b1-4bb4-a6db-f7125029e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully produced 1000 messages to 'green_trips' topic.\n",
      "Successfully produced 1000 messages to 'fhv_trips' topic.\n"
     ]
    }
   ],
   "source": [
    "produce_taxi_data_with_kafka_python(num_messages=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e373e80-ca8c-40a3-8c63-516001bd76a3",
   "metadata": {},
   "source": [
    "### Loading Kafka Messages into PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74b1f63d-bbea-4d07-bc42-22e333c713d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_raw = load_df_from_kafka(spark, topic=\"green_trips\")\n",
    "df_fhv_raw = load_df_from_kafka(spark, topic=\"fhv_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "906608b3-f953-4486-a41b-b186e85a40b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema before parsing:\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|[32]|[32 36 34 2C 20 3...|green_trips|        0|     0|2023-03-15 23:56:...|            0|\n",
      "|[32]|[39 37 2C 20 32 3...|green_trips|        0|     1|2023-03-15 23:56:...|            0|\n",
      "|[32]|[34 39 2C 20 32 3...|green_trips|        0|     2|2023-03-15 23:56:...|            0|\n",
      "|[32]|[31 38 39 2C 20 3...|green_trips|        0|     3|2023-03-15 23:56:...|            0|\n",
      "|[32]|[38 32 2C 20 32 3...|green_trips|        0|     4|2023-03-15 23:56:...|            0|\n",
      "|[32]|[34 39 2C 20 32 3...|green_trips|        0|     5|2023-03-15 23:56:...|            0|\n",
      "|[32]|[32 35 35 2C 20 3...|green_trips|        0|     6|2023-03-15 23:56:...|            0|\n",
      "|[31]|[37 36 2C 20 32 3...|green_trips|        0|     7|2023-03-15 23:56:...|            0|\n",
      "|[32]|[32 35 2C 20 32 3...|green_trips|        0|     8|2023-03-15 23:56:...|            0|\n",
      "|[32]|[38 35 2C 20 32 3...|green_trips|        0|     9|2023-03-15 23:56:...|            0|\n",
      "|[32]|[32 32 33 2C 20 3...|green_trips|        0|    10|2023-03-15 23:56:...|            0|\n",
      "|[32]|[31 32 39 2C 20 3...|green_trips|        0|    11|2023-03-15 23:56:...|            0|\n",
      "|[32]|[37 31 2C 20 32 3...|green_trips|        0|    12|2023-03-15 23:56:...|            0|\n",
      "|[32]|[38 35 2C 20 32 3...|green_trips|        0|    13|2023-03-15 23:56:...|            0|\n",
      "|[31]|[32 35 36 2C 20 3...|green_trips|        0|    14|2023-03-15 23:56:...|            0|\n",
      "|[31]|[38 30 2C 20 32 3...|green_trips|        0|    15|2023-03-15 23:56:...|            0|\n",
      "|[31]|[32 35 36 2C 20 3...|green_trips|        0|    16|2023-03-15 23:56:...|            0|\n",
      "|[32]|[32 35 35 2C 20 3...|green_trips|        0|    17|2023-03-15 23:56:...|            0|\n",
      "|[32]|[31 34 36 2C 20 3...|green_trips|        0|    18|2023-03-15 23:56:...|            0|\n",
      "|[32]|[31 34 36 2C 20 3...|green_trips|        0|    19|2023-03-15 23:56:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "After parsing:\n",
      "+------------+--------------------+---------------------+---------------+-------------+-----------+\n",
      "|PULocationID|lpep_pickup_datetime|lpep_dropoff_datetime|passenger_count|trip_distance|fare_amount|\n",
      "+------------+--------------------+---------------------+---------------+-------------+-----------+\n",
      "|         264| 2018-12-21 15:17:29|  2018-12-21 15:18:57|              5|          0.0|        3.0|\n",
      "|          97| 2019-01-01 00:10:16|  2019-01-01 00:16:32|              2|         0.86|        6.0|\n",
      "|          49| 2019-01-01 00:27:11|  2019-01-01 00:31:38|              2|         0.66|        4.5|\n",
      "|         189| 2019-01-01 00:46:20|  2019-01-01 01:04:54|              2|         2.68|       13.5|\n",
      "|          82| 2019-01-01 00:19:06|  2019-01-01 00:39:43|              1|         4.53|       18.0|\n",
      "|          49| 2019-01-01 00:12:35|  2019-01-01 00:19:09|              1|         1.05|        6.5|\n",
      "|         255| 2019-01-01 00:47:55|  2019-01-01 01:00:01|              1|         3.77|       13.5|\n",
      "|          76| 2019-01-01 00:12:47|  2019-01-01 00:30:50|              1|          4.1|       16.0|\n",
      "|          25| 2019-01-01 00:16:23|  2019-01-01 00:39:46|              1|         7.75|       25.5|\n",
      "|          85| 2019-01-01 00:58:02|  2019-01-01 01:19:02|              1|         3.68|       15.5|\n",
      "|         223| 2019-01-01 00:37:00|  2019-01-01 00:56:42|              1|         6.84|       22.0|\n",
      "|         129| 2019-01-01 00:13:48|  2019-01-01 00:21:00|              2|         1.15|        6.5|\n",
      "|          71| 2019-01-01 00:19:59|  2019-01-01 00:45:50|              1|         0.49|       15.5|\n",
      "|          85| 2019-01-01 00:57:57|  2019-01-01 01:20:10|              1|         3.61|       17.0|\n",
      "|         256| 2019-01-01 00:09:02|  2019-01-01 00:17:50|              1|          1.2|        7.5|\n",
      "|          80| 2019-01-01 00:22:12|  2019-01-01 00:25:29|              1|          0.5|        4.0|\n",
      "|         256| 2019-01-01 00:31:55|  2019-01-01 00:52:59|              1|          5.5|       19.5|\n",
      "|         255| 2019-01-01 00:30:20|  2019-01-01 00:54:19|              1|         5.01|       20.0|\n",
      "|         146| 2018-12-31 23:58:06|  2019-01-01 00:00:57|              1|         0.43|        4.0|\n",
      "|         146| 2019-01-01 00:40:17|  2019-01-01 00:50:23|              1|         2.72|       10.5|\n",
      "+------------+--------------------+---------------------+---------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"PULocationID\", T.IntegerType()),\n",
    "        T.StructField(\"lpep_pickup_datetime\", T.TimestampType()),\n",
    "        T.StructField(\"lpep_dropoff_datetime\", T.TimestampType()),\n",
    "        T.StructField(\"passenger_count\", T.IntegerType()),\n",
    "        T.StructField(\"trip_distance\", T.FloatType()),\n",
    "        T.StructField(\"fare_amount\", T.FloatType()),\n",
    "    ]\n",
    ")\n",
    "df_green = parse_ride_df_from_kafka(df_green_raw, green_schema)\n",
    "print(\"Schema before parsing:\")\n",
    "streaming_df_to_batch_df(df_green_raw, spark).show()\n",
    "print(\"After parsing:\")\n",
    "streaming_df_to_batch_df(df_green, spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f17ed621-4dc1-4341-b8df-380264e7de3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema before parsing:\n",
      "root\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- SR_Flag: boolean (nullable = true)\n",
      "\n",
      "After parsing:\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+\n",
      "|PUlocationID|DOlocationID|    pickup_datetime|   dropOff_datetime|dispatching_base_num|SR_Flag|\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+\n",
      "|        null|        null|2019-01-01 00:30:00|2019-01-01 02:51:55|              B00001|   null|\n",
      "|        null|        null|2019-01-01 00:45:00|2019-01-01 00:54:49|              B00001|   null|\n",
      "|        null|        null|2019-01-01 00:15:00|2019-01-01 00:54:52|              B00001|   null|\n",
      "|        null|        null|2019-01-01 00:19:00|2019-01-01 00:39:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:27:00|2019-01-01 00:37:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:48:00|2019-01-01 01:02:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:50:00|2019-01-01 00:59:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:51:00|2019-01-01 00:56:00|              B00008|   null|\n",
      "|        null|        null|2019-01-01 00:44:00|2019-01-01 00:58:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:19:00|2019-01-01 00:36:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:36:00|2019-01-01 00:49:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:26:00|2019-01-01 00:32:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:26:00|2019-01-01 00:36:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:58:00|2019-01-01 01:05:00|              B00009|   null|\n",
      "|        null|        null|2019-01-01 00:02:29|2019-01-02 00:25:30|              B00013|   null|\n",
      "|        null|        null|2019-01-01 00:01:33|2019-01-02 00:18:16|              B00013|   null|\n",
      "|        null|         265|2019-01-01 00:02:43|2019-01-01 00:10:14|              B00037|   null|\n",
      "|        null|         265|2019-01-01 00:26:02|2019-01-01 00:37:00|              B00037|   null|\n",
      "|        null|         265|2019-01-01 00:11:16|2019-01-01 00:25:41|              B00037|   null|\n",
      "|        null|         265|2019-01-01 00:33:45|2019-01-01 00:45:28|              B00037|   null|\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fhv_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"PUlocationID\", T.IntegerType()),\n",
    "        T.StructField(\"DOlocationID\", T.IntegerType()),\n",
    "        T.StructField(\"pickup_datetime\", T.TimestampType()),\n",
    "        T.StructField(\"dropOff_datetime\", T.TimestampType()),\n",
    "        T.StructField(\"dispatching_base_num\", T.StringType()),\n",
    "        T.StructField(\"SR_Flag\", T.BooleanType()),\n",
    "    ]\n",
    ")\n",
    "df_fhv = parse_ride_df_from_kafka(df_fhv_raw, fhv_schema)\n",
    "print(\"Schema before parsing:\")\n",
    "df_fhv.printSchema()\n",
    "print(\"After parsing:\")\n",
    "streaming_df_to_batch_df(df_fhv, spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80edca55-5607-4f69-ad07-b758800573d0",
   "metadata": {},
   "source": [
    "### Combining Streaming Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94cf79b2-b9dd-4a54-ba45-29f45120e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green.withColumnRenamed(\n",
    "    \"lpep_pickup_datetime\", \"pickup_datetime\"\n",
    ").withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "df_fhv = df_fhv.withColumnRenamed(\"PUlocationID\", \"PULocationID\").withColumnRenamed(\n",
    "    \"dropOff_datetime\", \"dropoff_datetime\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e48514-2aa8-4277-ad73-fb92fe774a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green.withColumn(\"service_type\", F.lit(\"green\").cast(\"string\"))\n",
    "df_fhv = df_fhv.withColumn(\"service_type\", F.lit(\"fhv\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0412b9-e331-4ede-a152-45431b384c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------------+-------------------+--------------------+-------+------------+--------------------+---------------+-------------+-----------+\n",
      "|PULocationID|DOlocationID|    pickup_datetime|   dropoff_datetime|dispatching_base_num|SR_Flag|service_type|           timestamp|passenger_count|trip_distance|fare_amount|\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+------------+--------------------+---------------+-------------+-----------+\n",
      "|        null|        null|2019-01-01 00:30:00|2019-01-01 02:51:55|              B00001|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:45:00|2019-01-01 00:54:49|              B00001|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:15:00|2019-01-01 00:54:52|              B00001|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:19:00|2019-01-01 00:39:00|              B00008|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:27:00|2019-01-01 00:37:00|              B00008|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:48:00|2019-01-01 01:02:00|              B00008|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:50:00|2019-01-01 00:59:00|              B00008|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:51:00|2019-01-01 00:56:00|              B00008|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:44:00|2019-01-01 00:58:00|              B00009|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:19:00|2019-01-01 00:36:00|              B00009|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:36:00|2019-01-01 00:49:00|              B00009|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:26:00|2019-01-01 00:32:00|              B00009|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:26:00|2019-01-01 00:36:00|              B00009|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:58:00|2019-01-01 01:05:00|              B00009|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:02:29|2019-01-02 00:25:30|              B00013|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|        null|2019-01-01 00:01:33|2019-01-02 00:18:16|              B00013|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|         265|2019-01-01 00:02:43|2019-01-01 00:10:14|              B00037|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|         265|2019-01-01 00:26:02|2019-01-01 00:37:00|              B00037|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|         265|2019-01-01 00:11:16|2019-01-01 00:25:41|              B00037|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "|        null|         265|2019-01-01 00:33:45|2019-01-01 00:45:28|              B00037|   null|         fhv|2023-03-16 00:02:...|           null|         null|       null|\n",
      "+------------+------------+-------------------+-------------------+--------------------+-------+------------+--------------------+---------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv_wm = df_fhv.withColumn(\"timestamp\", F.current_timestamp()).withWatermark(\n",
    "    \"timestamp\", \"1 hours\"\n",
    ")\n",
    "df_green_wm = df_green.withColumn(\"timestamp\", F.current_timestamp()).withWatermark(\n",
    "    \"timestamp\", \"1 hours\"\n",
    ")\n",
    "df_rides = df_fhv_wm.unionByName(df_green_wm, allowMissingColumns=True)\n",
    "streaming_df_to_batch_df(df_rides, spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97716cb-6d49-48e2-bd01-efd2a1e525d5",
   "metadata": {},
   "source": [
    "### Querying Streaming Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0abbf3a-5f0e-467c-9976-c47d8e2cc9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pu_locations(df, spark, query_name=\"count_pu_locs\"):\n",
    "    query = (\n",
    "        df.writeStream.queryName(query_name)\n",
    "        .format(\"memory\")\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    "    )\n",
    "    query.awaitTermination()\n",
    "    query_results = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            PULocationID, COUNT(PULocationID) AS count\n",
    "        FROM\n",
    "            {query_name}\n",
    "        GROUP BY\n",
    "            PULocationID\n",
    "        ORDER BY\n",
    "            count DESC\n",
    "        LIMIT 10;\n",
    "    \"\"\"\n",
    "    )\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30b65ecf-05ec-4199-8055-cd995a9cb638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PULocationID|count|\n",
      "+------------+-----+\n",
      "|          41|  169|\n",
      "|         181|  155|\n",
      "|           7|  143|\n",
      "|          42|  134|\n",
      "|         255|  133|\n",
      "|          25|  120|\n",
      "|          74|   88|\n",
      "|          82|   85|\n",
      "|         129|   81|\n",
      "|          75|   78|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_pu_locations(df_rides, spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ec29207-1cc5-42a2-ad01-abb14a49d342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add new data to Kafka topics:\n",
      "Successfully produced 500 messages to 'green_trips' topic.\n",
      "Successfully produced 500 messages to 'fhv_trips' topic.\n",
      "Re-run query:\n",
      "+------------+-----+\n",
      "|PULocationID|count|\n",
      "+------------+-----+\n",
      "|          41|  199|\n",
      "|         181|  188|\n",
      "|           7|  170|\n",
      "|          42|  158|\n",
      "|         255|  157|\n",
      "|          25|  142|\n",
      "|          74|  106|\n",
      "|          82|  100|\n",
      "|          75|   95|\n",
      "|         129|   93|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Add new data to Kafka topics:\")\n",
    "produce_taxi_data_with_kafka_python(num_messages=500)\n",
    "print(\"Re-run query:\")\n",
    "count_pu_locations(df_rides, spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079b8c4-c840-4c8a-98ca-fdb17fe07470",
   "metadata": {},
   "source": [
    "### Writing Straming Dataframes to a Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fcab559-e705-4dbd-84e0-88b7121078ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_taxis_topic = \"all_taxi\"\n",
    "cols_to_write = [\"service_type\", \"pickup_datetime\", \"dropoff_datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28b1dd63-0000-413e-b459-cdac78f1e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rides_kafka = prepare_df_for_kafka(df_rides, cols_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b998080-0043-4c35-8bef-3042ec78fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch = streaming_df_to_batch_df(df_rides_kafka, spark)\n",
    "write_batch_df_to_kafka(df_batch, all_taxis_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65e7b3f9-5d64-49a8-9988-dd88b2a77329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "+----+--------------------+--------+---------+------+--------------------+-------------+\n",
      "| key|               value|   topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+--------+---------+------+--------------------+-------------+\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|     0|2023-03-15 23:56:...|            0|\n",
      "|null|[66 68 76 2C 20 3...|all_taxi|        0|     1|2023-03-15 23:56:...|            0|\n",
      "|null|[66 68 76 2C 20 3...|all_taxi|        0|     2|2023-03-15 23:56:...|            0|\n",
      "|null|[66 68 76 2C 20 3...|all_taxi|        0|     3|2023-03-15 23:56:...|            0|\n",
      "|null|[66 68 76 2C 20 3...|all_taxi|        0|     4|2023-03-15 23:56:...|            0|\n",
      "|null|[66 68 76 2C 20 3...|all_taxi|        0|     5|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|     6|2023-03-15 23:56:...|            0|\n",
      "|null|[66 68 76 2C 20 3...|all_taxi|        0|     7|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|     8|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|     9|2023-03-15 23:56:...|            0|\n",
      "|null|[66 68 76 2C 20 3...|all_taxi|        0|    10|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|    11|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|    12|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|    13|2023-03-15 23:56:...|            0|\n",
      "|null|[66 68 76 2C 20 3...|all_taxi|        0|    14|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|    15|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|    16|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|    17|2023-03-15 23:56:...|            0|\n",
      "|null|[66 68 76 2C 20 3...|all_taxi|        0|    18|2023-03-15 23:56:...|            0|\n",
      "|null|[67 72 65 65 6E 2...|all_taxi|        0|    19|2023-03-15 23:56:...|            0|\n",
      "+----+--------------------+--------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all_taxi = load_df_from_kafka(spark, topic=all_taxis_topic)\n",
    "print(\"Schema:\")\n",
    "streaming_df_to_batch_df(df_all_taxi, spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86979ddb-29e3-4537-bbb5-3f8c8e83b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 9246\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {streaming_df_row_count(df_all_taxi, spark)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3fb41d6-2bcf-4966-a8e4-455032ac28c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query = write_streaming_df_to_kafka(df_rides_kafka, all_taxis_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a2fec51-3a6f-4f3c-8f02-966bea75a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 9246\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {streaming_df_row_count(df_all_taxi, spark)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4371d25c-1c66-40df-aa9b-c1bedfc19fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully produced 123 messages to 'green_trips' topic.\n",
      "Successfully produced 123 messages to 'fhv_trips' topic.\n",
      "Number of rows: 15984\n"
     ]
    }
   ],
   "source": [
    "produce_taxi_data_with_kafka_python(num_messages=123)\n",
    "# Wait until all new data has streamed into new topic:\n",
    "while write_query.status[\"isTriggerActive\"]:\n",
    "    time.sleep(0.1)\n",
    "print(f\"Number of rows: {streaming_df_row_count(df_all_taxi, spark)}\")\n",
    "write_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4061f5a2-92fd-4a81-a6d2-72ba41278545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/16 00:11:54 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.1.146 instead (on interface enp9s0)\n",
      "23/03/16 00:11:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.17\n",
      "Branch HEAD\n",
      "Compiled by user liangchi on 2023-02-10T19:57:40Z\n",
      "Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
